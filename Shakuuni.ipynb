{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Shakuuni.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1wt-DOnKDXLYA0flLJPa01NpSFf1MTHV7",
      "authorship_tag": "ABX9TyO7TqgKgG8SIjVEy8WAfUQ4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kamikater34/AutomaticTextSummarization/blob/master/Shakuuni.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRIgPQe1K120"
      },
      "source": [
        "https://github.com/Shakunni/Extractive-Text-Summarization/blob/master/extractive_summarizer.ipynb\n",
        "https://neptune.ai/blog/google-colab-dealing-with-files\n",
        "\n",
        "Luhn\n",
        "https://github.com/mohammadKhalifa/Luhn-s-summarizer/blob/master/main.py\n",
        "\n",
        "Gui Luhn\n",
        "https://github.com/abiraja2004/Automatic-Text-Summarizer-2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovHUoEwStYac",
        "outputId": "8156147c-7000-4b97-8b4d-ea0ccd53bdce"
      },
      "source": [
        "!pip install sklearn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMePxPn9q9ir",
        "outputId": "9ba9f30a-a8d1-48f7-801b-7701ab05d126"
      },
      "source": [
        "!pip install PyPDF2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.7/dist-packages (1.26.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UCBWPQVrO-g",
        "outputId": "8242f310-f865-42c7-916e-8a6f0c854627"
      },
      "source": [
        "!pip install docx2txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: docx2txt in /usr/local/lib/python3.7/dist-packages (0.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJ3H-9M645BB"
      },
      "source": [
        "import os \n",
        "\n",
        "# Set your working directory to a folder in your Google Drive. This way, if your notebook times out,\n",
        "# your files will be saved in your Google Drive!\n",
        "\n",
        "# the base Google Drive directory\n",
        "root_dir = \"/content/drive/My-Drive/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYBKfsidq4T5"
      },
      "source": [
        "# numpy library helps in working with arrays: array creation and manipulation\n",
        "# this implementation uses array for storing the matrices generated as 2-D arrays\n",
        "# PyPDF2 is a library used for reading the PDF files\n",
        "# docx2txt is the library used for reading Word documents \n",
        "# sys library has been used for printing the size of data structures used in the program\n",
        "import numpy as np\n",
        "import PyPDF2\n",
        "import docx2txt\n",
        "import sys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrjXr5Ecq5oA"
      },
      "source": [
        "# matplotlib is a library that is used to visualize the data by drawing graphs of matrix inputs\n",
        "# we will use it for drawing the matrices generated later in the program \n",
        "# %matplotlib inline is a command used to show the graphs in the jupyter notebook\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8yTnTvuq8mY"
      },
      "source": [
        "# networkx library helps in working with graphs ...\n",
        "# and later performing the PageRank algorithm ...\n",
        "# which is the crux of this implementation to find ...\n",
        "# the importance of each sentence using their 'rank' as a metric ...\n",
        "# rank, the output of the method textrank, is a measure of importance of sentences\n",
        "# this library has been used in the cell no. ()\n",
        "\n",
        "import networkx as nx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWs88CCQsQv_"
      },
      "source": [
        "# the PunktSentenceTokenizer library is being imported from the file punkt.py contained in package nltk.tokenize \n",
        "# this is used to tokenize the document into sentences\n",
        "\n",
        "# Tokenization: Tokenization is the process of demarcating and possibly classifying.. \n",
        "# sections of a string of input characters. \n",
        "# The resulting tokens are then passed on to some other form of processing. \n",
        "\n",
        "from nltk.tokenize.punkt import PunktSentenceTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWGrMQVKs9TO"
      },
      "source": [
        "# TfidfTransformer and CountVectorizer libraries are being imported\n",
        "\n",
        "# CountVectorizer: In this implementation, a CountVectorizer object is being created that ..\n",
        "# will be used for creating the document-term matrix\n",
        "\n",
        "# tFidTransformer: In this implementation,TfidfTransformer is used for executing the method fit_transform()... \n",
        "# which provides the output as a document-term matrix normalized (value 0-1) according to the TF-IDF\n",
        "# TF(Term Frequency): the no. of times a term(a word here) appears in the current document(single sentence here)\n",
        "# IDF(Inverse Document Frequency): the no. of times a term(a word here) appears in the entire corpus\n",
        "# Corpus: set of all sentences\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UH3y1waPtAN9"
      },
      "source": [
        "# we are going to show an example of how the method is working\n",
        "# first let's take the document as an input\n",
        "def readDoc():\n",
        "    name = input('Please input a file name: ') \n",
        "    print('You have asked for the document {}'.format(name))\n",
        "\n",
        "    # now read the type of document\n",
        "    if name.lower().endswith('.txt'):\n",
        "        choice = 1\n",
        "    elif name.lower().endswith('.pdf'):\n",
        "        choice = 2\n",
        "    else:\n",
        "        choice = 3\n",
        "        # print(name)\n",
        "    print(choice)\n",
        "    # Case 1: if it is a .txt file\n",
        "        \n",
        "    if choice == 1:\n",
        "        f = open(name, 'r')\n",
        "        document = f.read()\n",
        "        f.close()\n",
        "            \n",
        "    # Case 2: if it is a .pdf file\n",
        "    elif choice == 2:\n",
        "        pdfFileObj = open(name, 'rb')\n",
        "        pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
        "        pageObj = pdfReader.getPage(0)\n",
        "        document = pageObj.extractText()\n",
        "        pdfFileObj.close()\n",
        "    \n",
        "    # Case 3: none of the format\n",
        "    else:\n",
        "        print('Failed to load a valid file')\n",
        "        print('Returning an empty string')\n",
        "        document = ''\n",
        "    \n",
        "    print(type(document))\n",
        "    return document"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXd0IgwmvTRM"
      },
      "source": [
        "\n",
        "# the function used for tokenizing the sentences\n",
        "# tokenization of a sentence: '''provided in cell() above'''\n",
        "\n",
        "def tokenize(document):\n",
        "    # We are tokenizing using the PunktSentenceTokenizer\n",
        "    # we call an instance of this class as sentence_tokenizer\n",
        "    doc_tokenizer = PunktSentenceTokenizer()\n",
        "    \n",
        "    # tokenize() method: takes our document as input and returns a list of all the sentences in the document\n",
        "    \n",
        "    # sentences is a list containing each sentence of the document as an element\n",
        "    sentences_list = doc_tokenizer.tokenize(document)\n",
        "    return sentences_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPStP3_T4Dv2"
      },
      "source": [
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJeWgQcP3_Ia",
        "outputId": "09729615-41e8-43aa-e475-f53646cf3e87"
      },
      "source": [
        "cwd = os.getcwd()  # Get the current working directory (cwd)\n",
        "files = os.listdir(cwd)  # Get all the files in that directory\n",
        "print(\"Files in %r: %s\" % (cwd, files))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files in '/content': ['.config', '.ipynb_checkpoints', 'drive', 'sample_data']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "FyV7HUr2vYi4",
        "outputId": "e9d5dbed-1180-410d-9ad3-abe4ec9f5ef1"
      },
      "source": [
        "# reading a file and \n",
        "# printing the size of the file\n",
        "document = readDoc()\n",
        "print('The length of the file is:', end=' ')\n",
        "print(len(document))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please input a file name: story1.txt\n",
            "You have asked for the document story1.txt\n",
            "1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-3d2a36d056b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# reading a file and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# printing the size of the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdocument\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadDoc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The length of the file is:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-4da83aee54d1>\u001b[0m in \u001b[0;36mreadDoc\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchoice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mdocument\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'story1.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aveec1oBvbd6"
      },
      "source": [
        "# we want to tokenize the document for further processing\n",
        "# tokenizing the sentence means that we are creating a list of all the sentences of the document.\n",
        "# Need of tokenizing the document: Initially the document is in just a string format.\n",
        "# if we want to process the document, we need to store it in a data structure.\n",
        "# Tokenization of document into words is also possible, but we will go with the tokenizing with the sentences\n",
        "# Since we want to choose the most relevant sentences, we need to generate tokens of sentences only\n",
        "sentences_list = tokenize(document)\n",
        "\n",
        "# let us print the size of memory used by the list sentences\n",
        "print('The size of the list in Bytes is: {}'.format(sys.getsizeof(sentences_list)))\n",
        "\n",
        "# the size of one of the element of the list\n",
        "print('The size of the item 0 in Bytes is: {}'.format(sys.getsizeof(sentences_list[0])))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}